{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This ipynb will upload daily barchartacs options and futures files\n",
    "1. The notebook will - for both the options table and the futures table - find the last day that there is data, and upload all days from that day.\n",
    "2. If there is more than a months worth of data, you should use steps 1, 2 and 3 to do monthly file uploads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import argparse as ap\n",
    "import sys\n",
    "import os\n",
    "if  not './' in sys.path:\n",
    "    sys.path.append('./')\n",
    "if  not '../' in sys.path:\n",
    "    sys.path.append('../')\n",
    "\n",
    "from barchartacs import build_db\n",
    "from barchartacs import db_info\n",
    "import datetime\n",
    "import json\n",
    "import io\n",
    "import urllib.request\n",
    "from tqdm import tqdm,tqdm_notebook\n",
    "\n",
    "import ipdb\n",
    "import importlib\n",
    "# importlib.reload(db_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WRITE_TO_POSTGRES = True\n",
    "CONTRACT_LIST = ['ES','CL','NG']\n",
    "STRIKE_DIVISOR = 1\n",
    "CSV_TEMP_PATH_OPTIONS = './temp_folder/df_all_temp_options.csv'\n",
    "CSV_TEMP_PATH_FUTURES = './temp_folder/df_all_temp_futures.csv'\n",
    "DIVISOR_DICT = json.load(open('./divisor_dict.json','r'))\n",
    "OPTTAB = 'sec_schema.options_table'\n",
    "FUTTAB = 'sec_schema.underlying_table'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def psql_copy(pga,full_tablename,csv_temp_path,logger,write_to_postgres=False):\n",
    "    # first get column names in order as they appear in postgres\n",
    "    db_username = pga.username\n",
    "    \n",
    "    copy_cmd = f\"\\COPY {full_tablename} FROM '{csv_temp_path}' DELIMITER ',' CSV HEADER;\"\n",
    "    if db_username is not None and len(db_username)>0:\n",
    "        psql_cmd = f'sudo -u {db_username} psql -d sec_db -c \"CMD\"'\n",
    "    else:\n",
    "        psql_cmd = f'psql  -d sec_db -c \"CMD\"'\n",
    "    psql_cmd = psql_cmd.replace('CMD',copy_cmd)\n",
    "    if  write_to_postgres:  # double check !!!\n",
    "        logger.info(f'BEGIN executing psql COPY command: {psql_cmd}')\n",
    "        os.system(psql_cmd)\n",
    "        logger.info(f'END executing psql COPY command')\n",
    "    else:\n",
    "        print(psql_cmd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_request(url,dict_headers=None):\n",
    "    '''\n",
    "    Example:\n",
    "    1. get google page\n",
    "        text_of_webpage =  do_request('https://www.google.com')\n",
    "    2. get text file from barchart daily \n",
    "        barchart_daily_text_file = do_request('http://acs.barchart.com/mri/data/opv07059.csv',dict_headers={\"Authorization\": \"Basic myauthcode\"})\n",
    "\n",
    "    '''\n",
    "    if dict_headers is None:\n",
    "        req = urllib.request.Request(url)\n",
    "    else:\n",
    "        req = urllib.request.Request(url, headers=dict_headers)\n",
    "    f = urllib.request.urlopen(req)\n",
    "    alines = f.read()#.decode('utf-8')\n",
    "    return alines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_daily(commod_code_list,yyyymmdd=None,barchart_auth_code=None,text_file_path=None,\n",
    "               divisor_dict = DIVISOR_DICT):\n",
    "    '''\n",
    "    commod_code_list: like ['ES'] or ['CL','CB']\n",
    "    yyyymmdd: like 20190705.  It must be a date in the current month\n",
    "    Example:\n",
    "    df_this_day = build_daily(['ES'],20190705)\n",
    "    '''\n",
    "    tfp = './temp_folder/opv.txt' if text_file_path is None else text_file_path\n",
    "    ymd = yyyymmdd\n",
    "    if ymd is None:\n",
    "        # get yesterday\n",
    "        ymd = int((datetime.datetime.now()-datetime.timedelta(1)).strftime('%Y%m%d'))\n",
    "    bac = barchart_auth_code\n",
    "    if bac is None:\n",
    "        bac = open('./temp_folder/barchart_authcode.txt','r').read()\n",
    "    y = str(ymd)[3]\n",
    "    mm = str(ymd)[4:6]\n",
    "    dd = str(ymd)[6:8]\n",
    "    opv = 'opv' + mm + dd + y\n",
    "    url = f'http://acs.barchart.com/mri/data/{opv}.csv'\n",
    "    dict_header = {\"Authorization\": f\"Basic {bac}\"}\n",
    "    opvtxt = do_request(url,dict_headers=dict_header)\n",
    "    opvtxt = opvtxt.decode('utf-8')\n",
    "    open(tfp,'w').write(opvtxt)\n",
    "    builder = build_db.BuildDb(None,strike_divisor_dict=divisor_dict,\n",
    "                           contract_list=commod_code_list,write_to_database=False)\n",
    "    dft = builder.build_options_pg_from_csvs(tfp)\n",
    "    return dft\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Futures file for a single day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_barchart_acs_single_day_futures_df(yyyymmdd):\n",
    "    this_year = int(str(yyyymmdd)[0:4])\n",
    "    zero_year = int(str(yyyymmdd)[0:3])*10\n",
    "    y = str(this_year - zero_year)\n",
    "    mm = str(yyyymmdd)[4:6]\n",
    "    dd = str(yyyymmdd)[6:8]\n",
    "    fut_url = f'http://acs.barchart.com/mri/data/mrg{mm}{dd}{y}.txt'\n",
    "    # fut_name = 'fut' + mm + dd + y\n",
    "    bac = open('./temp_folder/barchart_authcode.txt','r').read()\n",
    "\n",
    "    dict_header = {\"Authorization\": f\"Basic {bac}\"}\n",
    "    fut_txt = do_request(fut_url,dict_headers=dict_header)\n",
    "    fut_txt = fut_txt.decode('utf-8').split()\n",
    "    header = ','.join(['contract','month_year','yymmdd','open','high','low','close','volume','open_interest'])\n",
    "    fut_lines = [header]+fut_txt\n",
    "    f = io.StringIO()\n",
    "    for fut_line in fut_lines:\n",
    "        f.write(fut_line+'\\n')\n",
    "    f.seek(0)\n",
    "    df_fut = pd.read_csv(f)\n",
    "    return df_fut\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_futures_daily(yyyymmdd,logger):\n",
    "    '''\n",
    "    ************* Step 1: get file from barchartacs  ************************\n",
    "    '''\n",
    "    df_temp = get_barchart_acs_single_day_futures_df(yyyymmdd)\n",
    "    \n",
    "    '''\n",
    "    ************* Step 2:  create DataFrame with column names of database *************\n",
    "    '''\n",
    "    isnas = df_temp.yymmdd.isna()\n",
    "    df_temp = df_temp[~isnas]\n",
    "    df_temp = df_temp[~df_temp.open_interest.isna()]\n",
    "    df_temp.volume = df_temp.volume.fillna(0)\n",
    "    df_temp = df_temp[df_temp.open.astype(str).str.count('\\.')<=1]\n",
    "    df_temp.index = list(range(len(df_temp)))\n",
    "    df_temp.loc[df_temp.month_year=='Y','month_year'] = '2099Z'\n",
    "    symbols = df_temp.contract + df_temp.month_year.str.slice(-1,)  + df_temp.month_year.str.slice(2,4)\n",
    "    settle_dates = ('20' + df_temp.yymmdd.astype(str)).astype(float).astype(int)\n",
    "    opens = df_temp.open.astype(float)\n",
    "    highs = df_temp.high.astype(float)\n",
    "    lows = df_temp.low.astype(float)\n",
    "    closes = df_temp.close.astype(float)\n",
    "    volumes = df_temp.volume.astype(int)\n",
    "    open_interests = df_temp.open_interest.astype(int)\n",
    "    df_final = pd.DataFrame({'symbol':symbols,\n",
    "        'settle_date':settle_dates,\n",
    "        'open':opens,\n",
    "        'high':highs,\n",
    "        'low':lows,\n",
    "        'close':closes,\n",
    "        'adj_close':closes,\n",
    "        'volume':volumes,\n",
    "        'open_interest':open_interests})\n",
    "    \n",
    "    # add month_num to df_final\n",
    "    df_monthnum = pd.read_csv('month_codes.csv')\n",
    "    dfu2 = df_final.copy()\n",
    "    dfu2['contract'] = dfu2.symbol.str.slice(0,-3)\n",
    "    dfu2['year'] = dfu2.symbol.apply(lambda s: 2000 + int(s[-2:]))\n",
    "    dfu2['month_code'] = dfu2.symbol.str.slice(-3,-2)\n",
    "    dfu3 = dfu2.merge(df_monthnum,on='month_code',how='inner')\n",
    "    \n",
    "    # Create adj_close\n",
    "    dfu3['yyyymm'] = dfu3.year*100+dfu3.month_num\n",
    "    dfu4 = dfu3[['contract','symbol','settle_date','yyyymm']]\n",
    "    dfu4['contract_num'] =dfu4[['contract','settle_date','yyyymm']].groupby(['contract','settle_date']).yyyymm.rank()\n",
    "    dfu4['contract_num'] = dfu4['contract_num'].astype(int)\n",
    "    dfu4 = dfu4.sort_values(['settle_date','contract','yyyymm'])\n",
    "    dfu4.index = list(range(len(dfu4)))\n",
    "    dfu5 = df_final.merge(dfu4[['symbol','settle_date','contract_num']],on=['symbol','settle_date'])\n",
    "    dfu5.index = list(range(len(dfu5)))\n",
    "    dfu5.open=dfu5.open.round(8)\n",
    "    dfu5.high=dfu5.high.round(8)\n",
    "    dfu5.low=dfu5.low.round(8)\n",
    "    dfu5.close=dfu5.close.round(8)\n",
    "    dfu5.adj_close = dfu5.adj_close.round(8)\n",
    "    \n",
    "    # #### Are there dupes??\n",
    "    ag = ['symbol','settle_date']\n",
    "    df_counts = dfu5[ag+['close']].groupby(ag,as_index=False).count()\n",
    "    dupes_exist  = len(df_counts[df_counts.close>1])>0\n",
    "    if dupes_exist:\n",
    "        msg = f'The dataframe to be written to the database has duplicate records. They will be dropped'\n",
    "        logger.warn(msg)\n",
    "        dfu5 = dfu5.drop_duplicates()\n",
    "        dfu5.index = list(range(len(dfu5)))\n",
    "        \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    ************* Step 4: Write data to a csv to be used by psql COPY *************\n",
    "    '''\n",
    "    col_tuple_list =   [('symbol','text'),('settle_date','integer'),('contract_num','integer'),\n",
    "         ('open','numeric'),('high','numeric'),('low','numeric'),('close','numeric'),\n",
    "         ('adj_close','numeric'),('volume','integer'),('open_interest','integer')]\n",
    "    col_list = [l[0] for l in col_tuple_list]\n",
    "    return dfu5[col_list]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dt_to_yyyymmdd(dt):\n",
    "    return int(str(dt)[0:4])*100*100 + int(str(dt)[5:7])*100 + int(str(dt)[8:10])\n",
    "\n",
    "def get_commod_list_from_max_settle_date(pga,tablename=FUTTAB):\n",
    "    commods_in_last_day_sql = f\"\"\"\n",
    "        with\n",
    "        f1 as\n",
    "        (\n",
    "            select max(settle_date) as maxdate from {tablename}\n",
    "        )    \n",
    "        select distinct substring(ft.symbol,1,2) commod from {tablename} ft\n",
    "        join f1 on f1.maxdate = ft.settle_date\n",
    "    \"\"\"\n",
    "    df_fut_commods_in_last_day = pga.get_sql(commods_in_last_day_sql)\n",
    "    return list(df_fut_commods_in_last_day.commod.values)\n",
    "\n",
    "def get_dates_to_fetch(pga,tablename,symbol_list=None):\n",
    "    \"\"\"\n",
    "    :param tablename: like OPTTAB or FUTTAB\n",
    "    This method will return a list of yyyymmdd's.  IT WILL NOT RETURN THE CURRENT DAY.\n",
    "    \"\"\"\n",
    "    t = datetime.datetime.now()\n",
    "    all_symbols = symbol_list\n",
    "    if all_symbols is None:\n",
    "        all_contracts = pga.get_sql(f\"select distinct symbol from {tablename}\")\n",
    "        all_symbols = [s[:-3] for s in all_contracts]\n",
    "    dict_sym_yyyymm_list = {}\n",
    "    for sym in all_symbols:\n",
    "        max_yyyymmdd = pga.get_sql(\n",
    "            f\"select max(settle_date) maxdate from {tablename} where substring(symbol,1,{len(sym)})='{sym}'\"\n",
    "        ).iloc[0].maxdate\n",
    "        max_year = int(str(max_yyyymmdd)[0:4])\n",
    "        max_month = int(str(max_yyyymmdd)[4:6])\n",
    "        max_day = int(str(max_yyyymmdd)[6:8])\n",
    "        max_dt = datetime.datetime(max_year,max_month,max_day)\n",
    "        num_days = (t - max_dt).days\n",
    "        dates_to_process = [max_dt + datetime.timedelta(n) for n in range(1,num_days)]\n",
    "        yyyymmdds_to_process = [dt_to_yyyymmdd(d) for d in dates_to_process]\n",
    "        dict_sym_yyyymm_list[sym] = yyyymmdds_to_process\n",
    "    return dict_sym_yyyymm_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute a range of days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    if 'ipykernel_launcher' in sys.argv[0]:\n",
    "        sys.argv = sys.argv[:1] + ['--contract_list','CL,ES,NG','--write_to_postgres',str(WRITE_TO_POSTGRES)]\n",
    "\n",
    "    parser =  ap.ArgumentParser()\n",
    "    parser.add_argument('--log_file_path',type=str,\n",
    "                        help='path to log file. Default = logfile.log',\n",
    "                        default = 'logfile.log')\n",
    "    parser.add_argument('--logging_level',type=str,\n",
    "                        help='log level.  Default = INFO',\n",
    "                        default = 'INFO')\n",
    "    parser.add_argument('--db_config_csv_path',type=str,\n",
    "                        help='path to the csv file that holds config_name,dburl,databasename,username,password info for the postgres db that you will update (default is ./postgres_info.csv',\n",
    "                        default=\"./postgres_info.csv\")\n",
    "    parser.add_argument('--config_name',type=str,\n",
    "                        help='value of the config_name column in the db config csv file (default is local',\n",
    "                        default=\"local\")\n",
    "    parser.add_argument('--contract_list',type=str,\n",
    "                        help='a comma delimited string of commodity codes.  Default = CL,ES,NG',\n",
    "                        default = 'CL,ES,NG')\n",
    "    parser.add_argument('--strike_divisor_json_path',type=str,\n",
    "                        help='if specified, a path to a json file that contains divisors for each commodity in contract_list',\n",
    "                        default = './divisor_dict.json')\n",
    "    parser.add_argument('--write_to_postgres',type=str,\n",
    "                        help='if True the data will be written to postgres.  Otherwise, a psql COPY command will be printed to the console.  Default=False',\n",
    "                        default=\"False\")\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # create logger and pga instance\n",
    "    logger = build_db.init_root_logger('logfile.log','INFO' )\n",
    "    pga = db_info.get_db_info(args.config_name, './postgres_info.csv')\n",
    "\n",
    "    # see if we are updating db\n",
    "    write_to_postgres = str(args.write_to_postgres).lower()=='true'\n",
    "    logger.info(f\"fetching commod lists for options and futures\")    \n",
    "\n",
    "    # get commodities to be processed\n",
    "    commods = args.contract_list.split(',')\n",
    "    # get dates to be processed for options, grouped by commod\n",
    "    print(f\"geting dates to process for {OPTTAB} symols: {commods}\")\n",
    "    dict_options_yyyymmdds_per_commod = get_dates_to_fetch(pga,OPTTAB,commods)\n",
    "    options_yyyymmdds_to_fetch = []\n",
    "    for yyyymmdds in dict_options_yyyymmdds_per_commod.values():\n",
    "        options_yyyymmdds_to_fetch.extend(yyyymmdds)\n",
    "    options_yyyymmdds_to_fetch = list(set(options_yyyymmdds_to_fetch))\n",
    "    \n",
    "    df_all_options = None\n",
    "    for yyyymmdd in tqdm_notebook(options_yyyymmdds_to_fetch):\n",
    "        logger.info(f'executing options build for yyyymmdd {yyyymmdd} at {datetime.datetime.now()}')\n",
    "\n",
    "        # build options\n",
    "        try:\n",
    "            df_temp = build_daily(commod_code_list=commods,yyyymmdd=yyyymmdd)\n",
    "            if df_all_options is None:\n",
    "                df_all_options = df_temp.copy()\n",
    "            else:\n",
    "                df_all_options = df_all_options.append(df_temp)\n",
    "                df_all_options.index = list(range(len(df_all_options)))\n",
    "        except Exception as e:\n",
    "            logger.warn(f'ERROR MAIN LOOP creating options: {str(e)}')\n",
    "\n",
    "    # get dates to be processed for futures\n",
    "    print(f\"geting dates to process for {FUTTAB} symols: {commods}\")\n",
    "    dict_futures_yyyymmdds_per_commod = get_dates_to_fetch(pga,FUTTAB,commods)            \n",
    "    futures_yyyymmdds_to_fetch = []\n",
    "    for yyyymmdds in dict_futures_yyyymmdds_per_commod.values():\n",
    "        futures_yyyymmdds_to_fetch.extend(yyyymmdds)\n",
    "    futures_yyyymmdds_to_fetch = list(set(futures_yyyymmdds_to_fetch))\n",
    "\n",
    "    df_all_futures = None\n",
    "    for yyyymmdd in tqdm_notebook(futures_yyyymmdds_to_fetch):\n",
    "        logger.info(f'executing futures build for yyyymmdd {yyyymmdd} at {datetime.datetime.now()}')\n",
    "        # build futures\n",
    "        # first get a list of commodity codes (like CL or ES) that are in the most recent day of the database\n",
    "        commods_in_last_day_sql = \"\"\"\n",
    "            select distinct substring(symbol,1,2) commod from {FUTTAB}\n",
    "            where settle_data = max(settle_date)\n",
    "        \"\"\"\n",
    "\n",
    "        try:\n",
    "            df_temp = build_futures_daily(yyyymmdd,logger)\n",
    "            if df_all_futures is None:\n",
    "                df_all_futures = df_temp.copy()\n",
    "            else:\n",
    "                df_all_futures = df_all_futures.append(df_temp)\n",
    "                df_all_futures.index = list(range(len(df_all_futures)))\n",
    "        except Exception as e:\n",
    "            logger.warn(f'ERROR MAIN LOOP creating futures: {str(e)}')\n",
    "    \n",
    "    # Now use dict_options_yyyymmdds_per_commod to get rid of those rows where the yyyymmdd for a commodity are already in the database\n",
    "    if df_all_options is not None:\n",
    "        df_all_options2 = pd.DataFrame()\n",
    "        for sym in dict_options_yyyymmdds_per_commod.keys():\n",
    "            good_yyyymmdds = dict_options_yyyymmdds_per_commod[sym]\n",
    "            c1 = df_all_options.symbol.str.slice(0,-3)==sym\n",
    "            c2 = df_all_options.settle_date.isin(good_yyyymmdds)\n",
    "            df_options_temp = df_all_options[(c1) & (c2)]\n",
    "            df_all_options2 = df_all_options2.append(df_options_temp,ignore_index=True)\n",
    "        # NOW WRITE THIS DATA FOR THIS YEAR\n",
    "        df_all_options2.to_csv(CSV_TEMP_PATH_OPTIONS,index=False)\n",
    "        if write_to_postgres:\n",
    "            logger.info(f\"MAIN LOOP: writing options data to database\")\n",
    "            abspath = os.path.abspath(CSV_TEMP_PATH_OPTIONS)\n",
    "            psql_copy(pga,OPTTAB,abspath,logger,write_to_postgres=WRITE_TO_POSTGRES)\n",
    "\n",
    "    # Now use dict_futures_yyyymmdds_per_commod to get rid of those rows where the yyyymmdd for a commodity are already in the database\n",
    "    if df_all_futures is not None:        \n",
    "        df_all_futures2 = pd.DataFrame()\n",
    "        for sym in dict_futures_yyyymmdds_per_commod.keys():\n",
    "            good_yyyymmdds = dict_futures_yyyymmdds_per_commod[sym]\n",
    "            c1 = df_all_futures.symbol.str.slice(0,-3)==sym\n",
    "            c2 = df_all_futures.settle_date.isin(good_yyyymmdds)\n",
    "            df_futures_temp = df_all_futures[(c1) & (c2)]\n",
    "            df_all_futures2 = df_all_futures2.append(df_futures_temp,ignore_index=True)\n",
    "\n",
    "        # for futures only use those commod codes in fut_contract_list\n",
    "        df_all_futures2.to_csv(CSV_TEMP_PATH_FUTURES,index=False)\n",
    "        if write_to_postgres:\n",
    "            logger.info(f\"MAIN LOOP: writing futures data to database\")\n",
    "            abspath = os.path.abspath(CSV_TEMP_PATH_FUTURES)\n",
    "            psql_copy(pga,FUTTAB,abspath,logger,write_to_postgres=WRITE_TO_POSTGRES)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook step_03_options_futures_table_daily_loader.ipynb to script\n",
      "[NbConvertApp] Writing 16502 bytes to step_03_options_futures_table_daily_loader.py\n"
     ]
    }
   ],
   "source": [
    "# !jupyter nbconvert --to script step_03_options_futures_table_daily_loader.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
