{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if  not os.path.abspath('./') in sys.path:\n",
    "    sys.path.append(os.path.abspath('./'))\n",
    "if  not os.path.abspath('../') in sys.path:\n",
    "    sys.path.append(os.path.abspath('../'))\n",
    "\n",
    "\n",
    "import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from barchartacs import schedule_it as sch\n",
    "import pandas_datareader.data as pdr\n",
    "import yfinance as yf\n",
    "import traceback\n",
    "import json\n",
    "import requests\n",
    "\n",
    "\n",
    "import pyarrow as pa\n",
    "import redis\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import schedule_it#@UnresolvedImport\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_port = 6379\n",
    "redis_db = redis.Redis(host = 'localhost',port=6379,db=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_to_yyyymmdd(d):\n",
    "    return int(d.year)*100*100 + int(d.month)*100 + int(d.day)\n",
    "\n",
    "def str_to_yyyymmdd(d,sep='-'):\n",
    "    try:\n",
    "        dt = datetime.datetime.strptime(str(d)[:10],f'%Y{sep}%m{sep}%d')\n",
    "    except:\n",
    "        return None\n",
    "    s = '%04d%02d%02d' %(dt.year,dt.month,dt.day)\n",
    "    return int(s)\n",
    "\n",
    "def str_to_date(d,sep='-'):\n",
    "    try:\n",
    "        dt = datetime.datetime.strptime(str(d)[:10],f'%Y{sep}%m{sep}%d')\n",
    "    except:\n",
    "        return None\n",
    "    return dt\n",
    "\n",
    "\n",
    "def fetch_history(symbol,dt_beg,dt_end):\n",
    "    df = yf.download(symbol, dt_beg, dt_end,threads=False)\n",
    "    # move index to date column, sort and recreate index\n",
    "    df['date'] = df.index\n",
    "    df = df.sort_values('date')\n",
    "    df.index = list(range(len(df)))\n",
    "    # make adj close the close\n",
    "    df = df.drop(['Adj Close'],axis=1)\n",
    "    cols = df.columns.values \n",
    "    cols_dict = {c:c[0].lower() + c[1:] for c in cols}\n",
    "    df = df.rename(columns = cols_dict)\n",
    "    df['settle_date'] = df.date.apply(str_to_yyyymmdd)\n",
    "    return df\n",
    "\n",
    "def get_port_info_values(syms):\n",
    "    names = syms if type(syms)==list else syms.tolist()\n",
    "    tickers = yf.Tickers(names)\n",
    "    dict_list = []\n",
    "    for n in tqdm.tqdm(names):\n",
    "        d = tickers.tickers[n].get_info()\n",
    "        d['symbol'] = n\n",
    "        dict_list.append(d)\n",
    "    df_info_values = pd.DataFrame(dict_list)\n",
    "    return df_info_values\n",
    "    \n",
    "def update_wf_port_info(syms):\n",
    "    try:\n",
    "#         names = syms if type(syms)==list else syms.tolist()\n",
    "#         tickers = yf.Tickers(names)\n",
    "#         dict_list = []\n",
    "#         for n in tqdm.tqdm(names):\n",
    "#             d = tickers.tickers[n].get_info()\n",
    "#             d['symbol'] = n\n",
    "#             dict_list.append(d)\n",
    "#         df_info_values = pd.DataFrame(dict_list)\n",
    "        df_info_values = get_port_info_values(syms)\n",
    "        info_values_key = 'wf_port_info_csv'\n",
    "        update_redis_df(info_values_key,df_info_values)\n",
    "    except Exception as e:\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "def update_redis_df(key,df):\n",
    "    context = pa.default_serialization_context()#@UndefinedVariable\n",
    "    redis_db.set(key, context.serialize(df).to_buffer().to_pybytes())\n",
    "\n",
    "\n",
    "def get_fmp_ratios(symbol):\n",
    "    ratios_url = f'https://financialmodelingprep.com/api/v3/quote/{symbol}?apikey=5959d0222350b6d05dbfe64794b6f093'\n",
    "    response = json.loads(requests.get(ratios_url).text)\n",
    "    return response\n",
    "\n",
    "def update_db(beg_sym=None,port_path=None):\n",
    "    syms = None\n",
    "    if port_path is not None:\n",
    "        syms = pd.read_csv(port_path).symbol.values\n",
    "    else:\n",
    "        sp_url = \"https://datahub.io/core/s-and-p-500-companies/r/constituents.csv\"\n",
    "        df_sp_members = pd.read_csv(sp_url)  \n",
    "        df_sp_members = df_sp_members.sort_values('Symbol')\n",
    "        if beg_sym is not None:\n",
    "            df_sp_members = df_sp_members[df_sp_members.Symbol>=beg_sym]\n",
    "            syms = df_sp_members.Symbol.values\n",
    "    syms = np.append(syms,['SPY','QQQ'])\n",
    "    data_end_date = datetime.datetime.now()\n",
    "    data_beg_date = data_end_date - relativedelta(years=5)\n",
    "    pe_values = []\n",
    "    closes = []\n",
    "    with tqdm.tqdm(syms,position=0,leave=True) as pbar:\n",
    "        for sym in pbar:\n",
    "            pbar.set_postfix_str(s=sym)\n",
    "            try:\n",
    "                df_temp = fetch_history(sym, data_beg_date, data_end_date)\n",
    "                update_redis_df(f'{sym}_csv',df_temp)\n",
    "            except Exception as e:\n",
    "                print(f\"ERROR on {sym}: {str(e)}\")\n",
    "        \n",
    "    update_wf_port_info(syms)\n",
    "\n",
    "def schedule_updates(t=8,unit='hour',beg_sym=None,port_path=None):\n",
    "    logger = schedule_it.init_root_logger(\"logfile.log\", \"INFO\")\n",
    "    while True:\n",
    "        logger.info(f\"scheduling update for {unit} {t}\")\n",
    "        sch = schedule_it.ScheduleNext(unit, t,logger = logger)\n",
    "        sch.wait()\n",
    "        logger.info(f\"updating history\")\n",
    "        update_db(beg_sym=beg_sym,port_path=port_path)\n",
    "        logger.info(f\"sleeping until next {t} {unit} before next scheduling\")\n",
    "        time.sleep(5*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__=='__main__':\n",
    "    t = 20 if len(sys.argv)<2 else int(sys.argv[1])\n",
    "    bs = None if len(sys.argv)<3 else sys.argv[2]\n",
    "    port_path = None if len(sys.argv)<4 else sys.argv[3]\n",
    "    unit = 'hour' if len(sys.argv)<5 else sys.argv[4]\n",
    "    schedule_updates(t=t,unit=unit,beg_sym=bs,port_path=port_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook redis_server_stock_data_update.ipynb to script\n",
      "[NbConvertApp] Writing 5003 bytes to redis_server_stock_data_update.py\n"
     ]
    }
   ],
   "source": [
    "# !jupyter nbconvert --to script redis_server_stock_data_update.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
